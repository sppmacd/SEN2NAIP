train:
  batch_size: 2
  gradient_accumulation_steps: 1
  optimizer: "adam"
  loss: "mse"
