train:
  batch_size: 2
  gradient_accumulation_steps: 16
  optimizer: "adam"
  loss: "mse"
